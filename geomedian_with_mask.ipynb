{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%matplotlib inline\n",
    "from matplotlib import pyplot as plt\n",
    "import numpy as np\n",
    "import xarray as xr\n",
    "\n",
    "import hdstats\n",
    "import odc.algo"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup local dask cluster"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "jupyter": {
     "source_hidden": true
    }
   },
   "outputs": [],
   "source": [
    "from datacube.utils.rio import configure_s3_access\n",
    "from datacube.utils.dask import start_local_dask\n",
    "import os\n",
    "import dask\n",
    "from dask.utils import parse_bytes\n",
    "\n",
    "# configure dashboard link to go over proxy\n",
    "dask.config.set({\"distributed.dashboard.link\":\n",
    "                 os.environ.get('JUPYTERHUB_SERVICE_PREFIX', '/')+\"proxy/{port}/status\"});\n",
    "\n",
    "# Figure out how much memory/cpu we really have (those are set by jupyterhub)\n",
    "mem_limit = int(os.environ.get('MEM_LIMIT', '0'))\n",
    "cpu_limit = float(os.environ.get('CPU_LIMIT', '0'))\n",
    "cpu_limit = int(cpu_limit) if cpu_limit > 0 else 4\n",
    "mem_limit = mem_limit if mem_limit > 0 else parse_bytes('8Gb')\n",
    "\n",
    "# leave 4Gb for notebook itself\n",
    "mem_limit -= parse_bytes('4Gb')\n",
    "\n",
    "# close previos client if any, so that one can re-run this cell without issues\n",
    "client = locals().get('client', None)\n",
    "if client is not None:\n",
    "    client.close()\n",
    "    del client\n",
    "    \n",
    "client = start_local_dask(n_workers=1,\n",
    "                          threads_per_worker=cpu_limit, \n",
    "                          memory_limit=mem_limit)\n",
    "display(client)\n",
    "\n",
    "# Configure GDAL for s3 access \n",
    "configure_s3_access(aws_unsigned=True,  # works only when reading public resources\n",
    "                    client=client);"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datacube import Datacube\n",
    "from odc.algo import fmask_to_bool, to_f32, from_float, xr_geomedian\n",
    "\n",
    "product = 'ga_s2a_ard_nbar_granule'\n",
    "product = (product, product.replace('s2a', 's2b'))\n",
    "\n",
    "dc = Datacube()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "region_code, time = '56HLK', ('2019-06-01', '2019-08-31') #('2019-06', '2019-11')\n",
    "\n",
    "dss = []\n",
    "for p in product:\n",
    "    dss += dc.find_datasets(product=p, \n",
    "                            region_code=region_code, \n",
    "                            time=time)\n",
    "\n",
    "\n",
    "tsm_dss = dc.find_datasets(product='s2_tsmask', \n",
    "                           time=time,\n",
    "                           region_code=region_code\n",
    "                           )\n",
    "len(dss), len(tsm_dss)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Do native load (lazy version with Dask)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data_bands = [\n",
    " #'nbar_coastal_aerosol',\n",
    " 'nbar_blue',\n",
    " 'nbar_green',\n",
    " 'nbar_red',\n",
    " #'nbar_red_edge_1',\n",
    " #'nbar_red_edge_2',\n",
    " #'nbar_red_edge_3',\n",
    " #'nbar_nir_1',\n",
    " #'nbar_nir_2',\n",
    " #'nbar_swir_2',\n",
    " #'nbar_swir_3',\n",
    "]\n",
    "\n",
    "mask_bands = ['fmask']\n",
    "\n",
    "xx = dc.load(product=dss[0].type.name,\n",
    "             output_crs=dss[0].crs,\n",
    "             resolution=(-10, 10),\n",
    "             align=(0, 0),\n",
    "             measurements=data_bands + mask_bands,\n",
    "             group_by='solar_day',\n",
    "             datasets=dss, \n",
    "             dask_chunks=dict(\n",
    "                 x=1000, \n",
    "                 y=1000)\n",
    "            )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xx"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tsm = dc.load(product='s2_tsmask',\n",
    "              like=xx.geobox,\n",
    "              datasets=tsm_dss, \n",
    "              dask_chunks=dict(\n",
    "                 x=1000,\n",
    "                 y=1000)\n",
    "             )\n",
    "tsm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Select a 3k by 3k subsection, to speed up testing\n",
    "if True:\n",
    "    _roi = dict(x=np.s_[0:3000], y=np.s_[-3000:])\n",
    "    xx = xx.isel(**_roi)\n",
    "    tsm = tsm.isel(**_roi)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Compute geomedian on data_bands\n",
    "1. Convert fmask to boolean: `True` - use, `False` - do not use\n",
    "2. Apply masking in native dtype for data bands only\n",
    "3. Convert to `float32` with scaling\n",
    "4. Reduce time dimension with geometric median\n",
    "5. Convert back to native dtype with scaling\n",
    "\n",
    "All steps are dask operations, so no actuall computation is done until `.compute()` is called."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fm_nocloud = fmask_to_bool(xx.fmask, ('water', 'snow', 'valid'))\n",
    "tsm_nocloud = fmask_to_bool(tsm.classification, ('valid',))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "nocloud = tsm_nocloud"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scale, offset = (1/10_000, 0)  # differs per product, aim for 0-1 values in float32\n",
    "\n",
    "xx_data = xx[data_bands]\n",
    "xx_clean = odc.algo.keep_good_only(xx_data, where=nocloud)\n",
    "xx_clean = to_f32(xx_clean, scale=scale, offset=offset)\n",
    "yy = xr_geomedian(xx_clean, \n",
    "                  num_threads=1,  # disable internal threading, dask will run several concurrently\n",
    "                  eps=0.2*scale,  # 1/5 pixel value\n",
    "                  nocheck=True)   # disable some checks inside geomedian library that use too much ram\n",
    "\n",
    "yy = from_float(yy, \n",
    "                dtype='int16', \n",
    "                nodata=-999, \n",
    "                scale=1/scale, \n",
    "                offset=-offset/scale)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Now we can run the computation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "yy = yy.compute()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Convert to RGBA and display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from odc.ui import to_rgba, to_png_data\n",
    "from IPython.display import Image\n",
    "\n",
    "rgba = to_rgba(yy, clamp=3000)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%time\n",
    "png_data = to_png_data(rgba.data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if max(rgba.shape) < 4000:\n",
    "    display(Image(png_data))\n",
    "else:\n",
    "    print('image too large to show')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "with open(f'rgba-{region_code}-s2ab-jja-xxx.png', 'wb') as f:\n",
    "    f.write(png_data)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "------------------------------------------------------------------"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.9"
  },
  "widgets": {
   "application/vnd.jupyter.widget-state+json": {
    "state": {},
    "version_major": 2,
    "version_minor": 0
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
